{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZs3p2rzcId6"
      },
      "source": [
        "## Assignment 3: Vision Transformer and Contrastive Representation Learning\n",
        "\n",
        "In this assignment, first, you are to implement [Vision Transformer](https://arxiv.org/pdf/2010.11929v2.pdf) (ViT) for image recognition. Then, you are utilize the implemented ViT to train a model using [Contrastive Representation Learning](https://arxiv.org/pdf/1503.03832.pdf), i.e. Triplet Loss.\n",
        "\n",
        "### Submission Guidelines\n",
        "The assignment codebase is split into two files. `vit_model.py` and `assignment3.ipynb`. In `vit_model.py`, incomplete codes for ViT and embedding model are provided, while in `assignment3.ipynb`, incomplete codes to iterate [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist) and initiate training and testing are provided in blocks. You are to complete the missing codeblock (Marked with comments and #TODO signs) and train the embedding representation model.\n",
        "\n",
        "In addition to PyTorch, [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning) is needed to help with the implementation of contrastive representation learning. You are strongly encouraged to go through the [README of the repo](https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/README.md), which contains the basic usage of the package. You are also strongly encouraged to study the example notebooks provided within the repo, at your own pace.\n",
        "\n",
        "To submit this assignment for grading, you must submit a compressed zipfile as `Assignment3_{YourCCID}.zip`, with three files inside: `vit_model.py`, `assignment3.ipynb`, and your trained embedding model as `vit_embeds.pt`.\n",
        "\n",
        "### Collaboration Policy\n",
        "This must be your own work. Do not share or look at the code of other students (whether they are inside or outside the class). Do not copy the code from the Internet, other than referring [PyTorch's official tutorials](https://pytorch.org/tutorials/), or the examples provided within [pytorch-metric-learning repo](https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/examples). You can talk to others in the class about solution ideas (but detailed enough that you are verbally sharing, hearing or seeing the code). You must cite whom you talked with in the comments of your programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVuYIttfcId8",
        "outputId": "498b288d-7276-456d-88db-fba3bf4d30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m92.2/115.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.3.0\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Make sure the external packages are installed in Colab\n",
        "!pip install pytorch-metric-learning\n",
        "!pip install faiss-gpu\n",
        "!pip install matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Bimwm7PcId9"
      },
      "outputs": [],
      "source": [
        "# Necessary imports for this assignment\n",
        "# NOTE: DO NOT MODIFY UNLESS NECESSARY\n",
        "import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop, CenterCrop\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import miners, losses, distances\n",
        "from pytorch_metric_learning.distances import LpDistance\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kH_Gwj-lcId-"
      },
      "outputs": [],
      "source": [
        "# Google Colab Patch\n",
        "use_colab = True\n",
        "if use_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import sys\n",
        "    # ----------------------------------------\n",
        "    dir = \"/content/drive/MyDrive/Colab Notebooks/CMPUT 328/ASSIGNMENT 3\"    # TODO: MODIFY THIS TO INDICATE THE PARENT FOLDER OF YOUR vit_model.py file\n",
        "    # ----------------------------------------\n",
        "\n",
        "\n",
        "from vit_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jzuFmnZPcId-"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# NOTE: Feel free to add & modify hyperparameters if needed\n",
        "num_epochs = 50\n",
        "batch_size = 25\n",
        "weight_decay = 1e-4\n",
        "lr = 1e-5       # TODO: Use appropriate LR with LR scheduler\n",
        "\n",
        "# ViT specifics\n",
        "image_size = 28\n",
        "in_channels = 1\n",
        "patch_size = 4\n",
        "hidden_size = 64\n",
        "layers = 6\n",
        "heads = 8\n",
        "embed_size = 64\n",
        "\n",
        "# Contrastive Learning specifics\n",
        "margin = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KjyHF1hcId-"
      },
      "source": [
        "### Part 1: Data Augmentation\n",
        "\n",
        "For the dataset, we will use the FashionMNIST dataset. For implmentation, you are explore the usage of different [data augmentation](https://pytorch.org/vision/master/auto_examples/transforms/plot_transforms_illustrations.html) techniques.\n",
        "\n",
        "**(1 out of 8)** You are to use at least 2 different types of data augmentation techniques (e.g. RandomHorizontalFlip) for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nJVJ6s4VcId-"
      },
      "outputs": [],
      "source": [
        "# Load FashionMNIST dataset\n",
        "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "num_classes = len(classes)\n",
        "\n",
        "tfm_train = Compose([\n",
        "    # TODO: Add your data augmentation HERE\n",
        "    # ####################\n",
        "\n",
        "    RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with a 50% probability\n",
        "    RandomCrop(size=(28, 28)),    # Randomly crop a square of size (28, 28)\n",
        "    CenterCrop(size=(28,28)),\n",
        "\n",
        "    # ####################\n",
        "    ToTensor(),\n",
        "    Normalize((0.5, ), (0.5, )),\n",
        "    ])\n",
        "\n",
        "tfm_test = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5, ), (0.5, )),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "Z6kYrimHcId_",
        "outputId": "5b2afd36-265b-43d5-e8b7-bebc5a56fbc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12254611.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 208442.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3929081.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 4016420.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# NOTE: Inspect your data augmentation\n",
        "def inverse_transform(\n",
        "    img_tensor: torch.Tensor,\n",
        "    ) -> np.ndarray:\n",
        "    \"\"\"Given a preprocessed image tensor, revert the normalization process and\n",
        "    convert the tensor back to a numpy image.\n",
        "    \"\"\"\n",
        "    inv_normalize = Normalize(mean=(-0.5/0.5, ), std=(1/0.5, ))\n",
        "    img_tensor = inv_normalize(img_tensor).permute(1, 2, 0)\n",
        "    img = np.uint8(255 * img_tensor.numpy())\n",
        "    return img\n",
        "\n",
        "# Get some random training images\n",
        "n_imgs = 5\n",
        "dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
        "indices = np.random.randint(0, len(dataset), size=(n_imgs, ))\n",
        "\n",
        "# Visualize with matplotlib\n",
        "for i, idx in enumerate(indices):\n",
        "    img_tensor, label = dataset[idx]\n",
        "    img = inverse_transform(img_tensor)\n",
        "    plt.subplot(1, n_imgs, i + 1)\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(classes[label])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZOxiKKwcId_"
      },
      "source": [
        "### Part 2: Vision Transformer\n",
        "**(2 out of 8)** For Part 1, you are to finish the implmentation of ViT, following the discussed lab instructions. Incomplete codes are provided in file `vit_model.py`.\n",
        "\n",
        "A `check_vit.py` test script is provided to inspect the correctness of your implementations. Make sure not to modify any code in the provided test script.\n",
        "\n",
        "Additionally, the code to train a classifier is provided below. If your ViT implementation is correct, a trained model should reach 88% accuracy and above with no issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ddwv85bbcId_"
      },
      "outputs": [],
      "source": [
        "# Train a Classifier\n",
        "# -----\n",
        "\n",
        "def train_classification_model():\n",
        "    # Base ViT\n",
        "    vit_model = ViT(\n",
        "        image_size=image_size,\n",
        "        patch_size=patch_size,\n",
        "        num_channels=in_channels,\n",
        "        hidden_size=hidden_size,\n",
        "        layers=layers,\n",
        "        heads=heads)\n",
        "\n",
        "    # Classifier\n",
        "    model_classifier = ClassificationHead(hidden_size=vit_model.hidden_size, num_classes=num_classes)\n",
        "    if torch.cuda.is_available():\n",
        "        vit_model = vit_model.cuda()\n",
        "        model_classifier = model_classifier.cuda()\n",
        "\n",
        "    # Use cross entropy\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Specify optimizer\n",
        "    parameters = list(vit_model.parameters()) + list(model_classifier.parameters())\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        parameters,\n",
        "        lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Evaluate at the end of each epoch\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, labels) in enumerate(train_loader):\n",
        "            vit_model.train()\n",
        "            model_classifier.train()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            feats = vit_model(x)\n",
        "            outputs = model_classifier(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # NOTE: Show train loss at the end of epoch\n",
        "            # Feel free to modify this to log more steps\n",
        "            if (i+1) % len(train_loader) == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "        # Evaluate at the end\n",
        "        test_acc = test_classification_model(vit_model, model_classifier)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state_dict = {\n",
        "                \"classifier\": model_classifier.state_dict(),\n",
        "                \"vit\": vit_model.state_dict(),\n",
        "                \"acc\": best_acc,\n",
        "            }\n",
        "            torch.save(state_dict, \"vit_classifier.pt\")\n",
        "            print(\"Best test acc:\", best_acc)\n",
        "        print()\n",
        "\n",
        "def test_classification_model(\n",
        "    vit_model: nn.Module,\n",
        "    model_classifier: nn.Module,\n",
        "    ):\n",
        "    # Test the model\n",
        "    vit_model.eval()\n",
        "    model_classifier.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            feats = vit_model(images)\n",
        "            outputs = model_classifier(feats)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print('Test Accuracy: {} %'.format(100 * correct / total))\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8CjHHTucId_"
      },
      "outputs": [],
      "source": [
        "\n",
        " # NOTE: Uncomment this to test your ViT implementation\n",
        "\n",
        "# train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
        "# test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# train_classification_model()\n",
        "\n",
        "# del train_dataset\n",
        "# del test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVxcZ1rbcId_"
      },
      "source": [
        "### Part 3: Contrastive Representation Learning\n",
        "In this part, you are to use [PyTorch-Metric-Learning](https://github.com/KevinMusgrave/pytorch-metric-learning) to implement triplet loss and learn an embedding representation model with ViT. Quick tutorials and notebooks are available in the repo.\n",
        "\n",
        "However, only a subset of data from the FashionMNIST training set (around 1/12) will be used to train the model. The code to subsample the full train dataset is provided in `sample_balanced_subset`. You must not modfiy the code in that block.\n",
        "\n",
        "To pass this part, you are to reach **60%** and above in Precision@1 metric scoring on the full FashionMNIST test dataset with your trained embedding model. The final model must be submitted along with your notebook with logging outputs. The test function is provided in `evaluate_at_end_epoch`.\n",
        "\n",
        "Furthermore, you are required to employ one of the learning rate schedulers, like [MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR), [Consine Annealing LR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR). Or you can implement your own learning rate adjusting strategy, as a function to training iteration or epoch, using [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR). The marking is distributed as follows:\n",
        "\n",
        "- (2 out of 8) Complete `train_embedding_model`. Add your selected learning rate scheduler, and train your embedding model\n",
        "- (0.5 out of 8) Complete `get_embeddings` to correctly perform inference with your embedding model.\n",
        "- (1 out of 8) Attach the notebook with full training logs (do not clear your notebook outputs when finished running), and your trained model file with **60%** and above in Precision@1 metric scoring, as `vit_embeds.pt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8lNxxXUcIeA",
        "outputId": "7a5c7f35-825f-4f7f-be7c-7c57ae4e1014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. samples in subset: 5000\n"
          ]
        }
      ],
      "source": [
        "# Sample a subset of training dataset\n",
        "# NOTE: sample an even number of samples per class\n",
        "# To migrate the problem of [class imbalancing]\n",
        "# (https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data).\n",
        "def sample_balanced_subset(\n",
        "    train_dataset,\n",
        "    n_samples_per_class: int = 50,\n",
        "    n_classes: int = 10\n",
        "    ):\n",
        "    maps = {i: [] for i in range(n_classes)}\n",
        "    for idx, (_, cls_idx) in enumerate(train_dataset):\n",
        "        if len(maps[cls_idx]) < n_samples_per_class:\n",
        "            maps[cls_idx].append(idx)\n",
        "\n",
        "    indices = []\n",
        "    for _, ind in maps.items():\n",
        "        indices += ind\n",
        "\n",
        "    train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
        "    print(\"Num. samples in subset:\", len(train_subset))\n",
        "    return train_subset\n",
        "\n",
        "# Subsample a small subset from the full train dataset\n",
        "full_train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
        "train_dataset = sample_balanced_subset(full_train_dataset, n_samples_per_class=500)      # DO NOT MODIFY THIS CODE\n",
        "\n",
        "# Full test set will still be used for testing\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_SlDIV0gcIeA"
      },
      "outputs": [],
      "source": [
        "# Train an embedding model\n",
        "# -----\n",
        "\n",
        "def train_embedding_model(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    ):\n",
        "    #########################\n",
        "    # Finish Your Code HERE\n",
        "    # #########################\n",
        "\n",
        "    # Dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Evaluate model per epoch, and keep track of the model performance at that time\n",
        "    best_prec = 0.0\n",
        "\n",
        "    # Triplet Loss with Semi-hard triples & l2 distance\n",
        "    # -------------------\n",
        "    # TODO: Specify distance, objective function (triplet loss), and your miner to sample triplet pairs\n",
        "\n",
        "    # distance = distances.LpDistance(normalize_embeddings = True)\n",
        "    distance = distances.LpDistance(p=2)\n",
        "    criterion = losses.TripletMarginLoss(margin=0.2, distance=distance)\n",
        "    miner = miners.TripletMarginMiner(type_of_triplets= \"semihard\")        # Make sure to use \"semihard\" triplets\n",
        "\n",
        "    # -------------------\n",
        "\n",
        "    # Precision to test embedding quality\n",
        "    accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
        "\n",
        "    # Models to train\n",
        "    # -------------------\n",
        "    # TODO: Specify your model(s) correctly here\n",
        "    # Don't forget to send them to cuda/gpu\n",
        "    vit_model = vit_model = ViT(\n",
        "        image_size=image_size,\n",
        "        patch_size=patch_size,\n",
        "        num_channels=in_channels,\n",
        "        hidden_size=hidden_size,\n",
        "        layers=layers,\n",
        "        heads=heads)\n",
        "\n",
        "    model_embedding = LinearEmbeddingHead(hidden_size=hidden_size,embed_size=embed_size)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        vit_model = vit_model.cuda()\n",
        "        model_embedding = model_embedding.cuda()\n",
        "\n",
        "    # -------------------\n",
        "\n",
        "    # Specify optimizer\n",
        "    # -------------------\n",
        "    parameters = list(vit_model.parameters()) + list(model_embedding.parameters())       # TODO: correctly specify the trainable parameters here\n",
        "\n",
        "    optimizer = optim.Adam(parameters, lr=0.001)        # TODO: Specify your optimizer\n",
        "\n",
        "    # Milestones = [5,7] has been selected as learning rate will be reduced by 0.1 at 5th and 7th epochs.\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,7], gamma=0.1)        # TODO: Specify your LR scheduler\n",
        "\n",
        "    # -------------------\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, labels) in enumerate(train_loader):\n",
        "            vit_model.train()\n",
        "            model_embedding.train()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            # -------------------\n",
        "            # TODO: Implement forward and backward pass\n",
        "            # TODO: Correctly update your LR with your selected LR scheduler\n",
        "            embeddings = model_embedding(vit_model(x))\n",
        "            loss = criterion(embeddings, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            # -------------------\n",
        "\n",
        "            # NOTE: Show train loss at the end of epoch\n",
        "            # Feel free to modify this to log more steps\n",
        "            if (i+1) % len(train_loader) == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "        # Evaluate at the end\n",
        "        best_prec = evaluate_at_end_epoch(vit_model, model_embedding, accuracy_calculator, best_prec, train_dataset, test_dataset)\n",
        "\n",
        "    # #########################\n",
        "\n",
        "def get_embeddings(\n",
        "    x: torch.Tensor,\n",
        "    vit_model: nn.Module,\n",
        "    model_embedding: nn.Module,\n",
        "    ) -> torch.Tensor:\n",
        "    \"\"\"Calculate embeddings for a batch of images.\n",
        "    \"\"\"\n",
        "    #########################\n",
        "    # Finish Your Code HERE\n",
        "    # #########################\n",
        "    # TODO: Correctly calculate x_embeds for a batch of\n",
        "    # images x with shape (batch_size, 3, h, w)\n",
        "\n",
        "    vit_model.eval()\n",
        "    model_embedding.eval()\n",
        "    if torch.cuda.is_available():\n",
        "      x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x_embeds = model_embedding(vit_model(x))\n",
        "    #########################\n",
        "\n",
        "    x_embeds = x_embeds.cpu()   # Cast to CPU\n",
        "    x_embeds = torch.nn.functional.normalize(x_embeds, p=2, dim=1)      # Extra Step: Normalize the embeddings\n",
        "    return x_embeds\n",
        "\n",
        "def get_embeddings_over_dataset(\n",
        "    dataset,\n",
        "    vit_model: nn.Module,\n",
        "    model_embedding: nn.Module,\n",
        "    ):\n",
        "    \"\"\"Loop through a full dataset and return all embeddings.\n",
        "    \"\"\"\n",
        "    # Create a loader on the go\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    X_embeds, Y = [], []\n",
        "    for i, (x, y) in enumerate(tqdm.tqdm(loader)):\n",
        "        x_embeds = get_embeddings(x, vit_model, model_embedding)\n",
        "        X_embeds.append(x_embeds)\n",
        "        Y.append(y)\n",
        "\n",
        "    X_embeds = torch.cat(X_embeds, dim=0)\n",
        "    Y = torch.cat(Y, dim=0)\n",
        "    return X_embeds, Y\n",
        "\n",
        "def test_embedding_model(\n",
        "    vit_model: nn.Module,\n",
        "    model_embedding: nn.Module,\n",
        "    accuracy_calculator,\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    ):\n",
        "    # Test the model\n",
        "    model_embedding.eval()\n",
        "\n",
        "    X_embeds, Y = get_embeddings_over_dataset(train_dataset, vit_model, model_embedding)\n",
        "    X_embeds_test, Y_test = get_embeddings_over_dataset(test_dataset, vit_model, model_embedding)\n",
        "    accuracies = accuracy_calculator.get_accuracy(\n",
        "        X_embeds_test, Y_test, X_embeds, Y, False\n",
        "    )\n",
        "    return accuracies[\"precision_at_1\"]\n",
        "\n",
        "def evaluate_at_end_epoch(\n",
        "    vit_model: nn.Module,\n",
        "    model_embedding: nn.Module,\n",
        "    accuracy_calculator,\n",
        "    best_prec: float,\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    ):\n",
        "    # Evaluate at the end\n",
        "    prec = test_embedding_model(vit_model, model_embedding, accuracy_calculator, train_dataset, test_dataset)\n",
        "    if prec > best_prec:\n",
        "        best_prec = prec\n",
        "        state_dict = {\n",
        "            \"embedding_head\": model_embedding.state_dict(),\n",
        "            \"vit\": vit_model.state_dict(),\n",
        "            \"precision@1\": prec,\n",
        "        }\n",
        "        torch.save(state_dict, \"vit_embeds.pt\")\n",
        "        print(\"Best Precision@1:\", best_prec)\n",
        "    print()\n",
        "    return best_prec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FnZPTY80_F9",
        "outputId": "2f7e04b0-c29f-4c10-ca21-ccfece99e724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [200/200], Loss: 0.1487\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 55.27it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 75.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6653\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/50], Step [200/200], Loss: 0.1843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 51.33it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 83.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6837000000000001\n",
            "\n",
            "Epoch [3/50], Step [200/200], Loss: 0.1922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 57.06it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 82.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6910000000000001\n",
            "\n",
            "Epoch [4/50], Step [200/200], Loss: 0.1505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.34it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6953\n",
            "\n",
            "Epoch [5/50], Step [200/200], Loss: 0.1763\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 65.57it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 70.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6973\n",
            "\n",
            "Epoch [6/50], Step [200/200], Loss: 0.1839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.98it/s]\n",
            "100%|██████████| 400/400 [00:06<00:00, 66.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.6986\n",
            "\n",
            "Epoch [7/50], Step [200/200], Loss: 0.1526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 57.48it/s]\n",
            "100%|██████████| 400/400 [00:06<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7052\n",
            "\n",
            "Epoch [8/50], Step [200/200], Loss: 0.1995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:05<00:00, 35.42it/s]\n",
            "100%|██████████| 400/400 [00:13<00:00, 30.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7142000000000001\n",
            "\n",
            "Epoch [9/50], Step [200/200], Loss: 0.1763\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 50.38it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 80.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7147\n",
            "\n",
            "Epoch [10/50], Step [200/200], Loss: 0.2063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 66.23it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7172000000000001\n",
            "\n",
            "Epoch [11/50], Step [200/200], Loss: 0.1814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.73it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 82.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7222000000000001\n",
            "\n",
            "Epoch [12/50], Step [200/200], Loss: 0.1886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 66.38it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 74.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7297\n",
            "\n",
            "Epoch [13/50], Step [200/200], Loss: 0.1938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 66.95it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 69.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.733\n",
            "\n",
            "Epoch [14/50], Step [200/200], Loss: 0.2280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.21it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [15/50], Step [200/200], Loss: 0.1876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.35it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 67.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7381000000000001\n",
            "\n",
            "Epoch [16/50], Step [200/200], Loss: 0.1873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 53.94it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 69.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7424000000000001\n",
            "\n",
            "Epoch [17/50], Step [200/200], Loss: 0.2067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 52.18it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7485\n",
            "\n",
            "Epoch [18/50], Step [200/200], Loss: 0.2391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 63.04it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 80.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [19/50], Step [200/200], Loss: 0.2128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.08it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [20/50], Step [200/200], Loss: 0.2420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.49it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 72.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7527\n",
            "\n",
            "Epoch [21/50], Step [200/200], Loss: 0.1829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 66.54it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [22/50], Step [200/200], Loss: 0.1521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.83it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7573000000000001\n",
            "\n",
            "Epoch [23/50], Step [200/200], Loss: 0.1715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 64.54it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 70.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7593000000000001\n",
            "\n",
            "Epoch [24/50], Step [200/200], Loss: 0.1900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 51.58it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 79.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7603000000000001\n",
            "\n",
            "Epoch [25/50], Step [200/200], Loss: 0.1719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 52.47it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [26/50], Step [200/200], Loss: 0.1909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 65.89it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7658\n",
            "\n",
            "Epoch [27/50], Step [200/200], Loss: 0.1341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 59.90it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [28/50], Step [200/200], Loss: 0.2250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.16it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 69.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [29/50], Step [200/200], Loss: 0.2259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.03it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 67.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.766\n",
            "\n",
            "Epoch [30/50], Step [200/200], Loss: 0.1273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.49it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7663000000000001\n",
            "\n",
            "Epoch [31/50], Step [200/200], Loss: 0.2866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 59.37it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 72.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7692\n",
            "\n",
            "Epoch [32/50], Step [200/200], Loss: 0.2429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:04<00:00, 49.81it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 80.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [33/50], Step [200/200], Loss: 0.2462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 57.15it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [34/50], Step [200/200], Loss: 0.1642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.67it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [35/50], Step [200/200], Loss: 0.2088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.65it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [36/50], Step [200/200], Loss: 0.2361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.08it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 70.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [37/50], Step [200/200], Loss: 0.2034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 63.08it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7726000000000001\n",
            "\n",
            "Epoch [38/50], Step [200/200], Loss: 0.2606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 66.40it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [39/50], Step [200/200], Loss: 0.2410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 61.12it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 72.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [40/50], Step [200/200], Loss: 0.1778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 50.12it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 82.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [41/50], Step [200/200], Loss: 0.2112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 53.87it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7749\n",
            "\n",
            "Epoch [42/50], Step [200/200], Loss: 0.2016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 69.23it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [43/50], Step [200/200], Loss: 0.2109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.97it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 82.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [44/50], Step [200/200], Loss: 0.2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.90it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 75.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [45/50], Step [200/200], Loss: 0.1768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 65.89it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 68.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [46/50], Step [200/200], Loss: 0.1966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 68.65it/s]\n",
            "100%|██████████| 400/400 [00:06<00:00, 65.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7757000000000001\n",
            "\n",
            "Epoch [47/50], Step [200/200], Loss: 0.1957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 65.09it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 70.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7758\n",
            "\n",
            "Epoch [48/50], Step [200/200], Loss: 0.2079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 53.03it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 80.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [49/50], Step [200/200], Loss: 0.1739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:03<00:00, 51.20it/s]\n",
            "100%|██████████| 400/400 [00:04<00:00, 81.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7759\n",
            "\n",
            "Epoch [50/50], Step [200/200], Loss: 0.1616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 67.06it/s]\n",
            "100%|██████████| 400/400 [00:05<00:00, 79.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Precision@1: 0.7764000000000001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Uncomment this to train your embedding model\n",
        "torch.cuda.empty_cache()    # Clean-up memory 1st\n",
        "train_embedding_model(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZopLsCcIeB"
      },
      "source": [
        "### Part 4: Application\n",
        "In the below examples, we perform inference with the trained embedding model using K-nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjc3_gKWcIeB",
        "outputId": "af3f728e-5ce2-4c87-ef95-390d20a030ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()    # Clean-up memory 1st\n",
        "\n",
        "# Load your trained model\n",
        "# Base ViT\n",
        "vit_model = ViT(\n",
        "    image_size=image_size,\n",
        "    patch_size=patch_size,\n",
        "    num_channels=in_channels,\n",
        "    hidden_size=hidden_size,\n",
        "    layers=layers,\n",
        "    heads=heads)\n",
        "\n",
        "# Embedding head\n",
        "model_embedding = LinearEmbeddingHead(hidden_size=vit_model.hidden_size, embed_size=embed_size)\n",
        "if torch.cuda.is_available():\n",
        "    vit_model = vit_model.cuda()\n",
        "    model_embedding = model_embedding.cuda()\n",
        "\n",
        "# Load saved checkpoint\n",
        "checkpoint = torch.load(\"vit_embeds.pt\")\n",
        "vit_model.load_state_dict(checkpoint[\"vit\"])\n",
        "model_embedding.load_state_dict(checkpoint[\"embedding_head\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fexgj9XacIeB"
      },
      "source": [
        "To quickly infer K-nearest neighbors, we construct a small bank, where the (image, label) pairs in the bank are sampled from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlcahsP8cIeB",
        "outputId": "832f50dc-9b4b-462b-b118-684c4cc96a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. samples in subset: 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 32.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# Sample a bank, which is a small subset of trainset\n",
        "bank = sample_balanced_subset(train_dataset, n_samples_per_class=25)\n",
        "X_bank_embeds, Y_bank = get_embeddings_over_dataset(bank, vit_model=vit_model, model_embedding=model_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1rnRE7ecIeB"
      },
      "source": [
        "Now, we perform the K-nearest neighbors, by comparing our sample against the embeddings in the bank.\n",
        "\n",
        "**(0.5 out of 8)** You are to finish the implementation of the function `retrieve_topk_nearest_neighbors_l2`, in order to acquire the demoing visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "5qPWoJl0cIeB",
        "outputId": "4d44147d-4ed4-4707-ca34-894e74fc43f3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x300 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Deep KNN\n",
        "def retrieve_topk_nearest_neighbors_l2(\n",
        "    query: torch.Tensor,\n",
        "    ref: torch.Tensor,\n",
        "    k: int = 5,\n",
        "    ):\n",
        "    \"\"\"Calculate L2 distance between query and ref embeddings.\n",
        "    Return the top-k scores and the indices of the top-k scores.\n",
        "    \"\"\"\n",
        "    #########################\n",
        "    # Finish Your Code HERE\n",
        "    # #########################\n",
        "    # TODO: Implement L2 distance and topk ops\n",
        "\n",
        "    distance = torch.sqrt(torch.sum((query-ref)**2, dim=1))  # This is Euclidean distance between pairs of query and reference vectors in PyTorch.\n",
        "\n",
        "    top_k_scores, indices = torch.topk(distance, k=k, largest=True)   # To find k smallest we use (largest=true) and distances are typically inversely proportional to similarity\n",
        "    # #########################\n",
        "\n",
        "    return top_k_scores, indices\n",
        "\n",
        "# Grab some samples in test set\n",
        "dataiter = iter(test_loader)\n",
        "X, Y = next(dataiter)\n",
        "X_embeds = get_embeddings(X, vit_model=vit_model, model_embedding=model_embedding)\n",
        "\n",
        "# One random sample\n",
        "idx = np.random.randint(0, X.shape[0])\n",
        "img, label, embed = X[idx], Y[idx], X_embeds[idx]\n",
        "\n",
        "# Get top-k nearest samples\n",
        "values, indices = retrieve_topk_nearest_neighbors_l2(embed.unsqueeze(0), X_bank_embeds)\n",
        "\n",
        "# Visualize results\n",
        "_, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
        "axes[0].imshow(inverse_transform(img), cmap=\"gray\")\n",
        "axes[0].set_title(classes[label])\n",
        "for i, ind in enumerate(indices):\n",
        "    img_bank, label_bank = bank[ind]\n",
        "    axes[i + 1].imshow(inverse_transform(img_bank), cmap=\"gray\")\n",
        "    axes[i + 1].set_title(\"{}\\nScore: {:.3f}\".format(classes[label_bank], values[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXjxINjkcIeC"
      },
      "source": [
        "The trained embedding model can also be used to train a new classifier.\n",
        "\n",
        "The intuition behind **Transfer Learning**, and a referring tutorial can be seen [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n",
        "\n",
        "**(1 out of 8)** Complete your code to train a classification head only, using your trained ViT embedding model. But you should not modify your trained ViT model weights in your `train_classification_model_head_only` function.\n",
        "\n",
        "A sample function to test with the classification head is provided, as `test_classification_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EbBOzeKLcIeC"
      },
      "outputs": [],
      "source": [
        "# Train a Classifier\n",
        "# -----\n",
        "\n",
        "def train_classification_model_head_only(\n",
        "    vit_model: nn.Module,\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    num_epochs: int = 5,\n",
        "    ):\n",
        "    #########################\n",
        "    # Finish Your Code HERE\n",
        "    # #########################\n",
        "\n",
        "    # Classifier\n",
        "    model_classifier = ClassificationHead(hidden_size=vit_model.hidden_size, num_classes=num_classes)\n",
        "    best_acc = 0.0\n",
        "    vit_model = ViT(\n",
        "        image_size=image_size,\n",
        "        patch_size=patch_size,\n",
        "        num_channels=in_channels,\n",
        "        hidden_size=hidden_size,\n",
        "        layers=layers,\n",
        "        heads=heads)\n",
        "    if torch.cuda.is_available():\n",
        "        vit_model = vit_model.cuda()\n",
        "        model_classifier = model_classifier.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Specify optimizer\n",
        "    parameters = list(vit_model.parameters()) + list(model_classifier.parameters())\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        parameters,\n",
        "        lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, labels) in enumerate(train_loader):\n",
        "            vit_model.train()\n",
        "            model_classifier.train()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            feats = vit_model(x)\n",
        "            outputs = model_classifier(feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # NOTE: Show train loss at the end of epoch\n",
        "            # Feel free to modify this to log more steps\n",
        "            if (i+1) % len(train_loader) == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "        # Evaluate at the end\n",
        "        test_acc = test_classification_model(vit_model, model_classifier)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            state_dict = {\n",
        "                \"classifier\": model_classifier.state_dict(),\n",
        "                \"vit\": vit_model.state_dict(),\n",
        "                \"acc\": best_acc,\n",
        "            }\n",
        "            torch.save(state_dict, \"vit_classifier.pt\")\n",
        "            print(\"Best test acc:\", best_acc)\n",
        "        print()\n",
        "\n",
        "    # #########################\n",
        "\n",
        "    # You should return the weights of your trained model, and the classification score (accuracy)\n",
        "    return {\"acc\": best_acc, \"vit\": vit_model.state_dict(), \"classifier\":model_classifier.state_dict()}\n",
        "\n",
        "def test_classification_model(\n",
        "    vit_model: nn.Module,\n",
        "    model_classifier: nn.Module,\n",
        "    ):\n",
        "    # Test the model\n",
        "    vit_model.eval()\n",
        "    model_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            feats = vit_model(images)\n",
        "            outputs = model_classifier(feats)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print('Test Accuracy: {} %'.format(100 * correct / total))\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFqMr8FrcIeC",
        "outputId": "d924e376-696e-4d75-a627-2efb3e9c2fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [2400/2400], Loss: 0.9794\n",
            "Test Accuracy: 70.62 %\n",
            "Best test acc: 0.7062\n",
            "\n",
            "Epoch [2/10], Step [2400/2400], Loss: 0.5602\n",
            "Test Accuracy: 75.42 %\n",
            "Best test acc: 0.7542\n",
            "\n",
            "Epoch [3/10], Step [2400/2400], Loss: 0.6460\n",
            "Test Accuracy: 77.46 %\n",
            "Best test acc: 0.7746\n",
            "\n",
            "Epoch [4/10], Step [2400/2400], Loss: 0.4169\n",
            "Test Accuracy: 78.92 %\n",
            "Best test acc: 0.7892\n",
            "\n",
            "Epoch [5/10], Step [2400/2400], Loss: 0.5542\n",
            "Test Accuracy: 80.13 %\n",
            "Best test acc: 0.8013\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train a deep classifier in 5 epochs\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=tfm_train)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=tfm_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "train_classification_model_head_only(vit_model, train_dataset, test_dataset)\n",
        "\n",
        "del train_dataset\n",
        "del test_dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
